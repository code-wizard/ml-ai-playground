{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from scipy.misc import imread\n",
    "import cv2\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to folder\n",
    "train_folder = \"images_background\"\n",
    "val_folder = \"images_evaluation\"\n",
    "save_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Image ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(path, n = 0):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    \n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        if alphabet == '.DS_Store':\n",
    "            continue\n",
    "        print(\"Loadin alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y, None]\n",
    "        alphabet_path = os.path.join(path, alphabet)\n",
    "        # every letter/category has it's own column in the array, so load seper\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            if letter == '.DS_Store':\n",
    "                # For macosx\n",
    "                continue\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images = []\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            \n",
    "            # read all the images in the current category\n",
    "            for filename in os.listdir(letter_path):\n",
    "                if filename == '.DS_Store':\n",
    "                    continue\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = plt.imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "                # edge case - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"Error - category_images:\", category_images)\n",
    "                    \n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X, y, lang_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadin alphabet: Gujarati\n",
      "Loadin alphabet: Korean\n",
      "Loadin alphabet: Arcadian\n",
      "Loadin alphabet: Malay_(Jawi_-_Arabic)\n",
      "Loadin alphabet: Grantha\n",
      "Loadin alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "Loadin alphabet: Balinese\n",
      "Loadin alphabet: Futurama\n",
      "Loadin alphabet: N_Ko\n",
      "Loadin alphabet: Burmese_(Myanmar)\n",
      "Loadin alphabet: Anglo-Saxon_Futhorc\n",
      "Loadin alphabet: Mkhedruli_(Georgian)\n",
      "Loadin alphabet: Latin\n",
      "Loadin alphabet: Braille\n",
      "Loadin alphabet: Sanskrit\n",
      "Loadin alphabet: Japanese_(hiragana)\n",
      "Loadin alphabet: Tagalog\n",
      "Loadin alphabet: Greek\n",
      "Loadin alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "Loadin alphabet: Japanese_(katakana)\n",
      "Loadin alphabet: Early_Aramaic\n",
      "Loadin alphabet: Hebrew\n",
      "Loadin alphabet: Tifinagh\n",
      "Loadin alphabet: Asomtavruli_(Georgian)\n",
      "Loadin alphabet: Armenian\n",
      "Loadin alphabet: Syriac_(Estrangelo)\n",
      "Loadin alphabet: Alphabet_of_the_Magi\n",
      "Loadin alphabet: Cyrillic\n",
      "Loadin alphabet: Bengali\n",
      "Loadin alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "X, y, c = loadimgs(train_folder)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the train sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"train\", X=X,c=c)\n",
    "np.save(\"train\", (X,c), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadin alphabet: Oriya\n",
      "Loadin alphabet: ULOG\n",
      "Loadin alphabet: Tengwar\n",
      "Loadin alphabet: Malayalam\n",
      "Loadin alphabet: Atlantean\n",
      "Loadin alphabet: Keble\n",
      "Loadin alphabet: Manipuri\n",
      "Loadin alphabet: Gurmukhi\n",
      "Loadin alphabet: Tibetan\n",
      "Loadin alphabet: Aurek-Besh\n",
      "Loadin alphabet: Ge_ez\n",
      "Loadin alphabet: Angelic\n",
      "Loadin alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "Loadin alphabet: Kannada\n",
      "Loadin alphabet: Avesta\n",
      "Loadin alphabet: Mongolian\n",
      "Loadin alphabet: Syriac_(Serto)\n",
      "Loadin alphabet: Atemayar_Qelisayer\n",
      "Loadin alphabet: Sylheti\n",
      "Loadin alphabet: Glagolitic\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "Xval, yval, cval = loadimgs(val_folder)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Validation tensors\n",
    "np.save(\"eval\", (Xval, cval), allow_pickle=True)\n",
    "# np.savez(\"eval\", Xval=Xval, cval=cval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del Xval, yval, cval, X, y, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, **kwargs):\n",
    "    \"\"\"\n",
    "     It was recommended to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc=0.0, scale=1e-2, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, **kwargs):\n",
    "    return np.random.normal(loc=0.5, scale=1e-2, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \n",
    "    # define the tensor for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # The CNN\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                     kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation=\"relu\",  input_shape=input_shape,\n",
    "                     bias_initializer=initialize_bias,\n",
    "                     kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation=\"relu\",  input_shape=input_shape,\n",
    "                     bias_initializer=initialize_bias,\n",
    "                     kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation=\"relu\",  input_shape=input_shape,\n",
    "                     bias_initializer=initialize_bias,\n",
    "                     kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation=\"sigmoid\", \n",
    "                    kernel_regularizer=l2(1e-3),\n",
    "                    kernel_initializer=initialize_weights,\n",
    "                    bias_initializer=initialize_bias\n",
    "                   ))\n",
    "    \n",
    "    # Generate the encoding \n",
    "    encoded_1 = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    #Add a customized layer layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_1, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1, activation=\"sigmoid\", bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the input with the outputs\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    return siamese_net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 4096)         38947648    input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 4096)         0           sequential_5[1][0]               \n",
      "                                                                 sequential_5[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            4097        lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,951,745\n",
      "Trainable params: 38,951,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((105,105, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.00006)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train tensors\n",
    "Xtrain, train_classes = np.load(\"train.npy\", allow_pickle=True)\n",
    "# training_data = np.load(\"train.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval, val_classes = np.load(\"eval.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, s=\"train\"):\n",
    "    \"\"\"\n",
    "        create batch of n pairs, half same class, half different class\n",
    "    \"\"\"\n",
    "    if s == \"train\":\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w , h = X.shape\n",
    "\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    \n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs = [np.zeros((batch_size, h, w, 1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets = np.zeros((batch_size, ))\n",
    "    \n",
    "    #make one half of it 1's and so 2nd hald of batch has same class\n",
    "    \n",
    "    targets [batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        \n",
    "        # pick images of same class for 1s half, different for 2nd\n",
    "        if 1 >= batch_size // 2:\n",
    "            category_2 = category\n",
    "        else:\n",
    "            # add a random number to the category modulo n classess to ensure 2nd  image has a different category\n",
    "            category_2 = (category * rng.randint(1, n_classes)) % n_classes\n",
    "        pairs[1][i,:,:,:] = X[category_2, idx_2].reshape(w, h, 1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genereate(batch_size, s='train'):\n",
    "    \"\"\"\n",
    "        a generator for batches, so model.fit_generator can be used\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        pairs, target = get_batch(batch_size, s)\n",
    "        yield(pairs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    " def make_oneshot_task(N, s=\"val\", language=None):\n",
    "        \"\"\"\n",
    "            Create pairs of test image support set for testing N way one-shot learning\n",
    "        \"\"\"\n",
    "        if s == \"train\":\n",
    "            X = Xtrain\n",
    "            categories = train_classes\n",
    "        else:\n",
    "            X = Xval\n",
    "            categories = val_classes\n",
    "        n_classes, n_examples, w, h = X.shape\n",
    "        \n",
    "        indices = np.random.randint(0, n_examples, size=(N,))\n",
    "        if language is not None:\n",
    "            low, high = categories[language]\n",
    "            if N > high - low:\n",
    "                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "            categories = np.random.choice(range(low, high), size=(N,), replace=False)\n",
    "        else:\n",
    "            categories = np.random.choice(range(n_classes), size=(N,), replace=False)\n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "        test_image = np.asarray([X[true_category, ex1,:,:]] * N).reshape(N, w, h, 1)\n",
    "        support_set = X[categories, indices, :, :]\n",
    "        support_set[0,:,:] = X[true_category, ex2]\n",
    "        support_set = support_set.reshape(N, w, h, 1)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        pairs = [test_image, support_set]\n",
    "        return pairs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s=\"val\", verbose=0):\n",
    "    \"\"\"\n",
    "        Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\n",
    "    \"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} randowm way one-shot learning task...\\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, target = make_oneshot_task(N, s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "        percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "# n_iter = 20000 # No of training iterations\n",
    "n_iter = 20 # No of training iterations\n",
    "N_way = 20 # how many classes for testing one-shot tasks\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-6f8d2ff5ed8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n ------------------ \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/opencv4/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/opencv4/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/opencv4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/opencv4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/opencv4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/opencv4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.virtualenvs/opencv4/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"----------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter +1):\n",
    "    (inputs, targets) = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------------ \\n\")\n",
    "        print(\"Time for {0} iterations: {1}mins\".format(i, (tiem.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss))\n",
    "        val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "model.load_weights(os.path.join(model_path, \"weights.20000.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing \n",
    "#Baselin model based on Nearest Neighbors using Euclidean Distance (L2 Distance)\n",
    "def nearest_neighbor_correct(pairs, targets):\n",
    "    \"\"\"\n",
    "        returns 1 if nearest neighbour gets the correct answer for a one-shot task given by (pairs, target)\n",
    "    \"\"\"\n",
    "    L2_distances = np.zeros_list(targets)\n",
    "    for i in range(len(targets)):\n",
    "        L2_distances[i] = np.sum(np.sqrt(pairs[0][i]**2 - pairs[1][i]**2))\n",
    "    if np.argmin(L2_distances) == np.argmax(targets):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn_accuracy(N_ways, n_trials):\n",
    "    \"\"\"Returns accuracy of NN approach\"\"\"\n",
    "    print(\"Evaluating nearest neighbor on {} unique {} way one-shot learning task\".format(n_trial, N_ways))\n",
    "    n_right = 0\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        pairs, targets = make_oneshot_task(N_ways, 'val')\n",
    "        correct = nearest_neighbor_correct(pairs, targets)\n",
    "        n_right += correct\n",
    "    return 100.0 * n_right / n_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways = np.arange(1, 20, 2)\n",
    "resume = False\n",
    "trial = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accs, train_accs,nn_accs = [], [], []\n",
    "for N in ways:    \n",
    "    val_accs.append(test_oneshot(model, N, trials, \"val\", verbose=True))\n",
    "    train_accs.append(test_oneshot(model, N, trials, \"train\", verbose=True))\n",
    "    nn_acc = test_nn_accuracy(N, trials)\n",
    "    nn_accs.append(nn_acc)\n",
    "    print (\"NN Accuracy = \", nn_acc)\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc, h , w, _ = X.shape\n",
    "    X = X.reshape(nc, h, w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneshot_task(pairs):\n",
    "    fig,(ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    ax1.matshow(pairs[0][0].reshape(105,105), cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs, targets = make_oneshot_task(16,\"train\",\"Sanskrit\")\n",
    "plot_oneshot_task(pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv4",
   "language": "python",
   "name": "opencv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
